apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  project: default
  source:
    chart: rook-ceph-cluster
    repoURL: https://charts.rook.io/release
    targetRevision: v1.16.5
    helm:
      valuesObject:
        configOverride: |
          [global]
          mon_osd_down_out_interval = 1800
        
        # ingress:
        #   dashboard:
        #     ingressClassName: nginx
        #     annotations:
        #       external-dns.alpha.kubernetes.io/target: ingress.ceph.impl.nl
        #       external-dns.alpha.kubernetes.io/cloudflare-proxied: "false"
        #       cert-manager.io/cluster-issuer: letsencrypt-production
        #     host:
        #       name: rook.ceph.impl.nl
        #     tls:
        #       - secretName: rook.ceph.impl.nl
        #         hosts:
        #           - rook.ceph.impl.nl
        monitoring:
          enabled: true

        toolbox:
          enabled: true

        cephClusterSpec:
          removeOSDsIfOutAndSafeToRemove: true
          mon:
            count: 3
            allowMultiplePerNode: false
          mgr:
            count: 2
            modules:
              - name: pg_autoscaler
                enabled: true
              - name: rook
                enabled: true
              - name: restful
                enabled: true
              - name: nfs
                enabled: false
              - name: cephadm
                enabled: false
              # - name: prometheus
              #   enabled: true
              - name: diskprediction_local
                enabled: true
              - name: insights
                enabled: true
          resources:
            mgr:
              limits:
                memory: "512Mi"
              requests:
                cpu: "500m"
                memory: "512Mi"
            mon:
              limits:
                memory: "1Gi"
              requests:
                cpu: "1000m"
                memory: "1Gi"
            osd:
              limits:
                memory: "2Gi"
              requests:
                cpu: "1000m"
                memory: "2Gi"

          # dashboard:
          #   enabled: false
          #   ssl: false
          #   prometheusEndpoint: http://prometheus-kube-prometheus-prometheus.prometheus:9090
          #   prometheusEndpointSSLVerify: false

          network:
            provider: host

          storage:
            allowDeviceClassUpdate: true
            allowOsdCrushWeightUpdate: true
            useAllNodes: true
            useAllDevices: true
            deviceFilter: ^nvme
            # nodes:
            #   - name: node65
            #     devices:
            #       - name: /dev/disk/by-id/nvme-TS4TMTE250S_I877750037
            #   - name: node66
            #     devices:
            #       - name: /dev/disk/by-id/nvme-TS4TMTE250S_I262140226
            #   - name: node67
            #     devices:
            #       - name: /dev/disk/by-id/nvme-TS2TMTE220S_I039920226
            #   - name: node68
            #     devices:
            #       - name: /dev/disk/by-id/nvme-TS2TMTE220S_I039920228

          crashCollector:
            disable: true
            daysToRetain: 7

          placement:
            all:
              tolerations:
                - key: "node.kubernetes.io/unschedulable"
                  operator: "Exists"
                  effect: "NoSchedule"
                - key: node-role.kubernetes.io/control-plane
                  operator: "Exists"
                  effect: "NoSchedule"
        cephBlockPools:
          - name: ceph-blockpool-big
            spec:
              replicated:
                size: 3
              enableRBDStats: false
            storageClass:
              enabled: false
          - name: ceph-blockpool-twice-replicated
            spec:
              replicated:
                size: 2
              enableRBDStats: false
            storageClass:
              enabled: false
        cephFileSystems:
          - name: ceph-filesystem
            # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
            spec:
              metadataPool:
                replicated:
                  size: 2
              dataPools:
                - failureDomain: host
                  replicated:
                    size: 2
                  # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
                  name: data0
              metadataServer:
                activeCount: 1
                activeStandby: true
                # resources:
                #   limits:
                #     memory: "4Gi"
                #   requests:
                #     cpu: "1000m"
                #     memory: "4Gi"
                priorityClassName: system-cluster-critical
            storageClass:
              enabled: false

        cephBlockPoolsVolumeSnapshotClass:
          enabled: false
        cephFileSystemVolumeSnapshotClass:
          enabled: false
        cephObjectStores: []
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - ServerSideApply=true
